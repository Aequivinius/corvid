[Paths]
data = ../data
vocab = vocab/
nlp = nlp/
devnull = /dev/null

[re]
prefix = \b(?:ante|anti|auto|co|de|dis|dys|extra|hetero|homo|hyper|hypo|inter|intra|juxta|mal|mid|mis|non|over|peri|post|pre|pro|re|sub|super|trans|un|under)
suffix = less\b
number = \d+
word = [^\W\d_]+
punct = [()+]
term-token = ${number}|${word}|${punct}

[Main]
log_level = WARNING

pointer_type = id
iter-mode = document
pointers = ${Paths:data}/pmids.txt
input_directory = ${Paths:data}/articles/txt/
article_format = pubmed
sentence_tokenizer = ${Paths:nlp}/punkt-sent_medline.pickle

export_format = bionlp.ann conll
fn_format_out = {base}.{fmt:.6}
conll_include = offsets
word_tokenizer = RegexTokenizer(r'([0-9a-zA-Z]+|[^0-9a-zA-Z\s])')

postfilter = builtin:frequentFP builtin:remove_sametype_submatches

[Termlist]
force_reload = false
normalize = lowercase greektranslit ise stem-porter
term-token = ${re:term-token}
stopwords = ${Paths:nlp}/stopwords-long.txt
